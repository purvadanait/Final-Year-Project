{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBc5axaeaUCmkIF6VoEVPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvadanait/Final-Year-Project/blob/main/inference_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1HfljAO0cho",
        "outputId": "928cb009-cc37-470b-d929-b0a97ebb2b43"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/cv2_utils.py\"\n",
        "!ls \"/content/drive/My Drive/deep_crack.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fguMH3mf1NCi",
        "outputId": "baaf9be6-51b6-4346-bd75-a8ea75aa4044"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/My Drive/cv2_utils.py'\n",
            "'/content/drive/My Drive/deep_crack.py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \"/content/drive/My Drive/cv2_utils.py\"\n",
        "!cat \"/content/drive/My Drive/deep_crack.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dkAOs7j1ply",
        "outputId": "0efaad8c-f285-4e59-d8d5-df9c22d09f58"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"cv2_utils.ipynb\n",
            "\n",
            "Automatically generated by Colaboratory.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1nM7TkHCvT0O8AzmfaLJFHpQhRuQSTfGD\n",
            "\"\"\"\n",
            "\n",
            "from scipy.spatial import distance as dist\n",
            "from imutils import perspective\n",
            "from imutils import contours\n",
            "import numpy as np\n",
            "import argparse\n",
            "import imutils\n",
            "import cv2\n",
            "from functools import reduce\n",
            "from scipy.interpolate import interp1d\n",
            "import math\n",
            "\n",
            "def midpoint(ptA, ptB):\n",
            "    return ((ptA[0] + ptB[0]) * 0.5, (ptA[1] + ptB[1]) * 0.5)\n",
            "\n",
            "def extract_bboxes(fused):\n",
            "    \"\"\"Compute bounding boxes from masks.\n",
            "    mask: [height, width]..\n",
            "    Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n",
            "    \"\"\"\n",
            "    mask = cv2.cvtColor(fused, cv2.COLOR_BGR2GRAY)\n",
            "    mask[mask < 40] = 0\n",
            "    mask[mask >= 40] = 1\n",
            "    mask = mask.reshape(256, 256, 1)\n",
            "    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n",
            "    for i in range(mask.shape[-1]):\n",
            "        m = mask[:, :, i]\n",
            "        # Bounding box.\n",
            "        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n",
            "        vertical_indicies = np.where(np.any(m, axis=1))[0]\n",
            "        if horizontal_indicies.shape[0]:\n",
            "            x1, x2 = horizontal_indicies[[0, -1]]\n",
            "            y1, y2 = vertical_indicies[[0, -1]]\n",
            "            # x2 and y2 should not be part of the box. Increment by 1.\n",
            "            x2 += 1\n",
            "            y2 += 1\n",
            "        else:\n",
            "            # No mask for this instance. Might happen due to\n",
            "            # resizing or cropping. Set bbox to zeros\n",
            "            x1, x2, y1, y2 = 0, 0, 0, 0\n",
            "        boxes[i] = np.array([y1, x1, y2, x2])\n",
            "    return boxes.astype(np.int32)\n",
            "\n",
            "def getContours(npImage, overlay_img, realHeight, realWidth, unit, confidence, angle_th=30):\n",
            "    # load the image, convert it to grayscale, and blur it slightly\n",
            "    image = npImage.copy()#cv2.imread(imagePath)\n",
            "    # image size\n",
            "    imgHeight = image.shape[0]\n",
            "    imgWidth = image.shape[1]\n",
            "\n",
            "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
            "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
            "    \n",
            "    # perform edge detection, then perform a dilation + erosion to\n",
            "    # close gaps in between object edges\n",
            "    edged = cv2.Canny(gray, 50, 80)\n",
            "    \n",
            "    edged = cv2.dilate(edged, None, iterations=1)\n",
            "    edged = cv2.erode(edged, None, iterations=1)\n",
            "    # find contours in the edge map\n",
            "    cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n",
            "                            cv2.CHAIN_APPROX_SIMPLE)\n",
            "    #cv2.drawContours(image=overlay_img, contours=cnts[0], contourIdx=-1, color=(0, 255, 0), thickness=2, lineType=cv2.LINE_AA)\n",
            "    cnts = imutils.grab_contours(cnts)\n",
            "    \n",
            "    # sort the contours from left-to-right and initialize the\n",
            "    # 'pixels per metric' calibration variable\n",
            "    (cnts, _) = contours.sort_contours(cnts)\n",
            "    pixelsPerMetricHeight = realHeight/imgHeight\n",
            "    pixelsPerMetricWidth = realWidth/imgWidth\n",
            "    #draw bounding box\n",
            "    y1, x1, y2, x2 = extract_bboxes(npImage)[0]\n",
            "    cv2.rectangle(overlay_img,(x1,y1),(x2, y2),(0,255,0),2)\n",
            "    # loop over the contours individually\n",
            "    \n",
            "    for c in cnts:\n",
            "        # if the contour is not sufficiently large, ignore it\n",
            "        if cv2.contourArea(c) < 100:\n",
            "            continue\n",
            "        # compute the rotated bounding box of the contour\n",
            "        orig = overlay_img.copy()\n",
            "        box = cv2.minAreaRect(c)\n",
            "        box = cv2.boxPoints(box)\n",
            "        box = np.array(box, dtype=\"int\")\n",
            "        # order the points in the contour such that they appear\n",
            "        # in top-left, top-right, bottom-right, and bottom-left\n",
            "        # order, then draw the outline of the rotated bounding\n",
            "        # box\n",
            "        box = perspective.order_points(box)\n",
            "\n",
            "        # unpack the ordered bounding box, then compute the midpoint\n",
            "        # between the top-left and top-right coordinates, followed by\n",
            "        # the midpoint between bottom-left and bottom-right coordinates\n",
            "        (tl, tr, br, bl) = box\n",
            "        (tltrX, tltrY) = midpoint(tl, tr)\n",
            "        (blbrX, blbrY) = midpoint(bl, br)\n",
            "        # compute the midpoint between the top-left and top-right points,\n",
            "        # followed by the midpoint between the top-righ and bottom-right\n",
            "        (tlblX, tlblY) = midpoint(tl, bl)\n",
            "        (trbrX, trbrY) = midpoint(tr, br)\n",
            "\n",
            "        # draw lines between the midpoints\n",
            "        #cv2.line(orig, (int(tltrX), int(tltrY)), (int(blbrX), int(blbrY)),\n",
            "                 #(255, 0, 0), 1)\n",
            "        cv2.line(orig, (int(tlblX), int(tlblY)), (int(trbrX), int(trbrY)),\n",
            "                 (0, 0, 255), 1)\n",
            "        \n",
            "\n",
            "        top_p = min([(int(tlblX), int(tlblY)), (int(trbrX), int(trbrY))], key=lambda x : x[1])\n",
            "        bot_p = max([(int(tlblX), int(tlblY)), (int(trbrX), int(trbrY))], key=lambda x : x[1])\n",
            "        D_ad = ((top_p[1] - bot_p[1]) ** 2 + (top_p[0] - bot_p[0])**2) ** 0.5 + 1e-7\n",
            "    \n",
            "        P1 = min(top_p, bot_p, key=lambda x:x[0])\n",
            "        P2 = max(top_p, bot_p, key=lambda x:x[0])\n",
            "        slope = (P1[1] - P2[1]) / (P2[0] - P1[0]) if (P2[0] - P1[0]) != 0 else 0\n",
            "        cat = ''\n",
            "        angle = 0        \n",
            "        if slope > 0:\n",
            "            angle = np.arccos((top_p[0] - bot_p[0])/D_ad) * 180 / math.pi\n",
            "            cv2.putText(orig, \"angle={:.1f}\".format(angle),\n",
            "            (max(top_p[0]-100, 0), top_p[1] + 15), cv2.FONT_HERSHEY_DUPLEX,\n",
            "            0.45, (0, 0, 255), 1)\n",
            "        else:\n",
            "            angle = np.arccos((bot_p[0] - top_p[0])/D_ad) * 180 / math.pi  \n",
            "            cv2.putText(orig, \"angle={:.1f}\".format(angle),\n",
            "            (top_p[0], top_p[1] + 15), cv2.FONT_HERSHEY_DUPLEX,\n",
            "            0.45, (0, 0, 255), 1)\n",
            "        \n",
            "\n",
            "        # compute the Euclidean distance between the midpoints\n",
            "        dA = dist.euclidean((tltrX, tltrY), (blbrX, blbrY))\n",
            "        dB = dist.euclidean((tlblX, tlblY), (trbrX, trbrY))\n",
            "        length = cv2.arcLength(c, True) / 2. * pixelsPerMetricWidth\n",
            "        M = cv2.moments(c)\n",
            "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
            "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
            "    \n",
            "        # width\n",
            "        mask = gray.copy()\n",
            "        mask[mask < 40] = 0\n",
            "        width = cv2.countNonZero(mask[cY][:])\n",
            "        right_most_x = np.max(np.nonzero(mask[cY][:]))\n",
            "        left_most_x = np.min(np.nonzero(mask[cY][:]))\n",
            "        cv2.line(orig, (int(left_most_x), int(cY)), (int(right_most_x), int(cY)),\n",
            "                 (0, 0, 255), 1)\n",
            "        width *= pixelsPerMetricWidth \n",
            "        # compute the size of the object\n",
            "        dimA = dA * pixelsPerMetricHeight\n",
            "        dimB = dB * pixelsPerMetricWidth\n",
            "        if angle < angle_th:\n",
            "            cat +='H'\n",
            "            cv2.putText(orig, \"L={:.1f}\".format(length) + unit,\n",
            "            (int(tltrX), int(tltrY) + 40), cv2.FONT_HERSHEY_DUPLEX,\n",
            "            0.45, (0, 0, 255), 1)\n",
            "            cv2.putText(orig, \"W={:.1f}\".format(width) + unit,\n",
            "            (int(tltrX), int(tltrY) + 55), cv2.FONT_HERSHEY_DUPLEX,\n",
            "            0.45, (0, 0, 255), 1)\n",
            "        else:\n",
            "            cat += 'V'\n",
            "            cv2.putText(orig, \"L={:.1f}\".format(length) + unit,\n",
            "            (int(tltrX), int(tltrY)), cv2.FONT_HERSHEY_DUPLEX,\n",
            "            0.45, (0, 0, 255), 1)\n",
            "            cv2.putText(orig, \"W={:.1f}\".format(width) + unit,\n",
            "            (int(tltrX), int(tltrY) + 15), cv2.FONT_HERSHEY_DUPLEX,\n",
            "            0.45, (0, 0, 255), 1)\n",
            "        if slope > 0:\n",
            "            cat += 'L'\n",
            "        else:\n",
            "            cat += 'R'\n",
            "        cv2.putText(orig, \"Crack {:.2f}%\".format(confidence.item()*100) + \"   cat=\"+ cat,  (x1, max(0, y1-5)), cv2.FONT_HERSHEY_SIMPLEX,  0.45, (36,255,12), 1)\n",
            "          \n",
            "        return orig\n",
            "\n",
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"deep_crack.ipynb\n",
            "\n",
            "Automatically generated by Colaboratory.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1tkFI3u1FdKyqWJmY80hjmP_dGFCb3qV4\n",
            "\"\"\"\n",
            "\n",
            "from torch import nn\n",
            "import torch\n",
            "import torch.nn.functional as F\n",
            "\n",
            "def Conv3X3(in_, out):\n",
            "    return torch.nn.Conv2d(in_, out, 3, padding=1)\n",
            "\n",
            "\n",
            "class ConvRelu(nn.Module):\n",
            "    def __init__(self, in_, out):\n",
            "        super().__init__()\n",
            "        self.conv = Conv3X3(in_, out)\n",
            "        self.activation = torch.nn.ReLU(inplace=True)\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.conv(x)\n",
            "        x = self.activation(x)\n",
            "        return x\n",
            "\n",
            "class Down(nn.Module):\n",
            "\n",
            "    def __init__(self, nn):\n",
            "        super(Down,self).__init__()\n",
            "        self.nn = nn\n",
            "        self.maxpool_with_argmax = torch.nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
            "\n",
            "    def forward(self,inputs):\n",
            "        down = self.nn(inputs)\n",
            "        unpooled_shape = down.size()\n",
            "        outputs, indices = self.maxpool_with_argmax(down)\n",
            "        return outputs, down, indices, unpooled_shape\n",
            "\n",
            "class Up(nn.Module):\n",
            "\n",
            "    def __init__(self, nn):\n",
            "        super().__init__()\n",
            "        self.nn = nn\n",
            "        self.unpool=torch.nn.MaxUnpool2d(2,2)\n",
            "\n",
            "    def forward(self,inputs,indices,output_shape):\n",
            "        outputs = self.unpool(inputs, indices=indices, output_size=output_shape)\n",
            "        outputs = self.nn(outputs)\n",
            "        return outputs\n",
            "\n",
            "class Fuse(nn.Module):\n",
            "\n",
            "    def __init__(self, nn, scale):\n",
            "        super().__init__()\n",
            "        self.nn = nn\n",
            "        self.scale = scale\n",
            "        self.conv = Conv3X3(64,1)\n",
            "\n",
            "    def forward(self,down_inp,up_inp):\n",
            "        outputs = torch.cat([down_inp, up_inp], 1)\n",
            "        outputs = F.interpolate(outputs, scale_factor=self.scale, mode='bilinear')\n",
            "        outputs = self.nn(outputs)\n",
            "\n",
            "        return self.conv(outputs)\n",
            "\n",
            "class DeepCrack(nn.Module):\n",
            "\n",
            "    def __init__(self, num_classes=1000):\n",
            "        super(DeepCrack, self).__init__()\n",
            "\n",
            "        self.down1 = Down(torch.nn.Sequential(\n",
            "            ConvRelu(3,64),\n",
            "            ConvRelu(64,64),\n",
            "        ))\n",
            "\n",
            "        self.down2 = Down(torch.nn.Sequential(\n",
            "            ConvRelu(64,128),\n",
            "            ConvRelu(128,128),\n",
            "        ))\n",
            "\n",
            "        self.down3 = Down(torch.nn.Sequential(\n",
            "            ConvRelu(128,256),\n",
            "            ConvRelu(256,256),\n",
            "            ConvRelu(256,256),\n",
            "        ))\n",
            "\n",
            "        self.down4 = Down(torch.nn.Sequential(\n",
            "            ConvRelu(256, 512),\n",
            "            ConvRelu(512, 512),\n",
            "            ConvRelu(512, 512),\n",
            "        ))\n",
            "\n",
            "        self.down5 = Down(torch.nn.Sequential(\n",
            "            ConvRelu(512, 512),\n",
            "            ConvRelu(512, 512),\n",
            "            ConvRelu(512, 512),\n",
            "        ))\n",
            "\n",
            "        self.up1 = Up(torch.nn.Sequential(\n",
            "            ConvRelu(64, 64),\n",
            "            ConvRelu(64, 64),\n",
            "        ))\n",
            "\n",
            "        self.up2 = Up(torch.nn.Sequential(\n",
            "            ConvRelu(128, 128),\n",
            "            ConvRelu(128, 64),\n",
            "        ))\n",
            "\n",
            "        self.up3 = Up(torch.nn.Sequential(\n",
            "            ConvRelu(256, 256),\n",
            "            ConvRelu(256, 256),\n",
            "            ConvRelu(256, 128),\n",
            "        ))\n",
            "\n",
            "        self.up4 = Up(torch.nn.Sequential(\n",
            "            ConvRelu(512, 512),\n",
            "            ConvRelu(512, 512),\n",
            "            ConvRelu(512, 256),\n",
            "        ))\n",
            "\n",
            "        self.up5 = Up(torch.nn.Sequential(\n",
            "            ConvRelu(512, 512),\n",
            "            ConvRelu(512, 512),\n",
            "            ConvRelu(512, 512),\n",
            "        ))\n",
            "\n",
            "        self.fuse5 = Fuse(ConvRelu(512 + 512, 64), scale=16)\n",
            "        self.fuse4 = Fuse(ConvRelu(512 + 256, 64), scale=8)\n",
            "        self.fuse3 = Fuse(ConvRelu(256 + 128, 64), scale=4)\n",
            "        self.fuse2 = Fuse(ConvRelu(128 + 64, 64), scale=2)\n",
            "        self.fuse1 = Fuse(ConvRelu(64 + 64, 64), scale=1)\n",
            "\n",
            "        self.final = Conv3X3(5,1)\n",
            "\n",
            "    def forward(self,inputs):\n",
            "\n",
            "        # encoder part\n",
            "        out, down1, indices_1, unpool_shape1 = self.down1(inputs)\n",
            "        out, down2, indices_2, unpool_shape2 = self.down2(out)\n",
            "        out, down3, indices_3, unpool_shape3 = self.down3(out)\n",
            "        out, down4, indices_4, unpool_shape4 = self.down4(out)\n",
            "        out, down5, indices_5, unpool_shape5 = self.down5(out)\n",
            "\n",
            "        # decoder part\n",
            "        up5 = self.up5(out, indices=indices_5, output_shape=unpool_shape5)\n",
            "        up4 = self.up4(up5, indices=indices_4, output_shape=unpool_shape4)\n",
            "        up3 = self.up3(up4, indices=indices_3, output_shape=unpool_shape3)\n",
            "        up2 = self.up2(up3, indices=indices_2, output_shape=unpool_shape2)\n",
            "        up1 = self.up1(up2, indices=indices_1, output_shape=unpool_shape1)\n",
            "\n",
            "        fuse5 = self.fuse5(down_inp=down5,up_inp=up5)\n",
            "        fuse4 = self.fuse4(down_inp=down4, up_inp=up4)\n",
            "        fuse3 = self.fuse3(down_inp=down3, up_inp=up3)\n",
            "        fuse2 = self.fuse2(down_inp=down2, up_inp=up2)\n",
            "        fuse1 = self.fuse1(down_inp=down1, up_inp=up1)\n",
            "\n",
            "        output = self.final(torch.cat([fuse5,fuse4,fuse3,fuse2,fuse1],1))\n",
            "\n",
            "        return output, fuse5, fuse4, fuse3, fuse2, fuse1\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    inp = torch.randn((1,3,512,512))\n",
            "\n",
            "    model = DeepCrack()\n",
            "\n",
            "    out = model(inp)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive\")"
      ],
      "metadata": {
        "id": "F6IYoOZr1zYE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2_utils"
      ],
      "metadata": {
        "id": "O_YoHpwZ2M-w"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive\")"
      ],
      "metadata": {
        "id": "rVN7BvRA9qNK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deep_crack"
      ],
      "metadata": {
        "id": "qv9Rt8vw8_aw"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "7APXGahVtsR6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from cv2_utils import getContours\n",
        "import torchvision.transforms as transforms\n",
        "from deep_crack import DeepCrack"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
        "    Parameters:\n",
        "        input_image (tensor) --  the input image tensor array\n",
        "        imtype (type)        --  the desired type of the converted numpy array\n",
        "    \"\"\"\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array\n",
        "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
        "    else:  # if it is a numpy array, do nothing\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype) "
      ],
      "metadata": {
        "id": "uQwNu7IPt1zd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bytes_to_array(b: bytes) -> np.ndarray:\n",
        "    np_bytes = BytesIO(b)\n",
        "    return np.load(np_bytes, allow_pickle=True)"
      ],
      "metadata": {
        "id": "l4_tukGD-YMe"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(bytesImg, dim=(256, 256)):    #Decode Bytes to array\n",
        "    img_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                                transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                                                       (0.5, 0.5, 0.5))])\n",
        "    img = np.fromstring(bytesImg, np.uint8)\n",
        "    img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # adjust the image size\n",
        "    w, h = dim\n",
        "    if w > 0 or h > 0:\n",
        "        img = cv2.resize(img, (w, h), interpolation=cv2.INTER_CUBIC)\n",
        "    \n",
        "    # apply the transform to both A and B\n",
        "    img = img_transforms(Image.fromarray(img.copy()))   \n",
        "    return img    "
      ],
      "metadata": {
        "id": "gHs5Xdty-1VR"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(opt, cp_path='pretrained_net_G.pth'):\n",
        "    model = DeepCrackModel(opt)      # create a model given opt.model and other options\n",
        "    checkpoint = torch.load(cp_path)\n",
        "    if hasattr(model.netG, 'module'):\n",
        "        model.netG.module.load_state_dict(checkpoint, strict=False)\n",
        "    else:\n",
        "        model.netG.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "pRL7STYc-79C"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlay(\n",
        "    image: np.ndarray,\n",
        "    mask: np.ndarray,\n",
        "    color = (255, 0, 0),\n",
        "    alpha: float = 0.5, \n",
        "    resize = (256, 256)\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Combines image and its segmentation mask into a single image.\n",
        "    \n",
        "    Params:\n",
        "        image: Training image.\n",
        "        mask: Segmentation mask.\n",
        "        color: Color for segmentation mask rendering.\n",
        "        alpha: Segmentation mask's transparency.\n",
        "        resize: If provided, both image and its mask are resized before blending them together.\n",
        "    \n",
        "    Returns:\n",
        "        image_combined: The combined image.\n",
        "        \n",
        "    \"\"\"\n",
        "    color = np.asarray(color).reshape(1, 1, 3)\n",
        "    colored_mask = np.expand_dims(mask, 0).repeat(3, axis=2)\n",
        "    masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
        "    image_overlay = masked.filled()\n",
        "    \n",
        "    if resize is not None:\n",
        "        image = cv2.resize(image, resize)\n",
        "        image_overlay = cv2.resize(image_overlay, resize)\n",
        "    \n",
        "    image_combined = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
        "    \n",
        "    return image_combined"
      ],
      "metadata": {
        "id": "AxwdFVuj_BFc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, bytesImg, dim, unit):\n",
        "    #print(img_path) \n",
        "    \n",
        "    image = read_image(bytesImg) #Read Array\n",
        "    # batchify\n",
        "    image = image.unsqueeze(0)\n",
        "    # hacky way to pass ground truth label\n",
        "    model.set_input({'image': image, 'label': torch.zeros_like(image), 'A_paths':''}) \n",
        "    model.test()           # run inference\n",
        "    visuals = model.get_current_visuals()  # get image results\n",
        "    confidence = visuals['fused'].max()\n",
        "\n",
        "    # fused for final prediction\n",
        "    for key in visuals.keys():\n",
        "        visuals[key] = tensor2im(visuals[key])\n",
        "        \n",
        "    h, w, _ = visuals['fused'].shape\n",
        "    fused = Image.fromarray(visuals['fused'])\n",
        "    fused = np.array(fused, dtype='uint8')\n",
        "    realHeight=dim[1]\n",
        "    realWidth=dim[0]\n",
        "\n",
        "    mask = cv2.cvtColor(fused, cv2.COLOR_BGR2GRAY)\n",
        "    mask[mask < 90] = 0\n",
        "    mask[mask >= 90] = 255\n",
        "    cnts = cv2.findContours(mask, cv2.RETR_EXTERNAL,\n",
        "                            cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    \n",
        "    overlay_img = overlay(tensor2im(image), mask, alpha=0)\n",
        "    cv2.drawContours(image=overlay_img, contours=cnts[0], contourIdx=-1, color=(0, 255, 0), thickness=1, lineType=cv2.LINE_AA)\n",
        "    contour_img = getContours(fused, overlay_img, realHeight, realWidth, unit, confidence)\n",
        "\n",
        "    return contour_img if contour_img is not None else overlay_img, visuals"
      ],
      "metadata": {
        "id": "clobyytD_Ex3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fvEDqQGv_FZd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}